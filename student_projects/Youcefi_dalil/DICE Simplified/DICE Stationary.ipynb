{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "eps = 0.0001  \n",
    "\n",
    "\n",
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "\n",
    "sess_path = None\n",
    "\n",
    "data_path =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def initialize_nn_weight(dim):\n",
    "    \"\"\"Initialize neural network weight or bias variable.\n",
    "    Args:\n",
    "        dim <list>: dimensions of weight matrix.\n",
    "                    - if len(dim) == 1: initializes bias,\n",
    "                    - if len(dim) == 2: initializes weight.\n",
    "    Returns:\n",
    "        Tensor variable of normally initialized data.\n",
    "    \"\"\"\n",
    "    t_stnd = tf.sqrt(tf.cast(dim[0], tf.float32)) * 10\n",
    "    return tf.Variable(tf.random_normal(tf.cast(dim, tf.int32)) / t_stnd, trainable=True)\n",
    "\n",
    "\n",
    "def random_mini_batches(X, minibatch_size=10, seed=0):\n",
    "    \"\"\"Generate random minibatches from X.\n",
    "    Args:\n",
    "        X <array>: Input data to be mini-batched.\n",
    "        minibatch_size <int>: mini-batch size.\n",
    "        seed <int>: seed.\n",
    "    Returns:\n",
    "        List of mini-batches generated from X.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle X\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "\n",
    "    # Step 2: Partition shuffled_X. Minus the end case.\n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    num_complete_minibatches = int(math.floor(m / minibatch_size))\n",
    "\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[(k * minibatch_size):((k+1) * minibatch_size), :]\n",
    "        mini_batch = (mini_batch_X)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000 \n",
    "len_episodes = 10240\n",
    "epochs_per_episode = 20\n",
    "minibatch_size = 512\n",
    "num_minibatches = int(len_episodes / minibatch_size)\n",
    "\n",
    "\n",
    "#Â Neural network architecture parameters\n",
    "num_input_nodes = 7  # Dimension of extended state space \n",
    "num_hidden_nodes = [100, 200]  # Dimension of hidden layers\n",
    "num_output_nodes = (3)  # Output dimension (Investment, Mitigation and Value function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a placeholder for X, the input data for the neural network, which corresponds\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_input_nodes))\n",
    "\n",
    "\n",
    "\n",
    "W1 = initialize_nn_weight([num_input_nodes, num_hidden_nodes[0]])\n",
    "W2 = initialize_nn_weight([num_hidden_nodes[0], num_hidden_nodes[1]])\n",
    "W3 = initialize_nn_weight([num_hidden_nodes[1], num_output_nodes])\n",
    "\n",
    "b1 = initialize_nn_weight([num_hidden_nodes[0]])\n",
    "b2 = initialize_nn_weight([num_hidden_nodes[1]])\n",
    "b3 = initialize_nn_weight([num_output_nodes])\n",
    "# We take a softmax activiation function which is more stable for minimizing the loss\n",
    "def nn_predict(X):\n",
    "    hidden_layer1 = tf.nn.softmax(tf.add(tf.matmul(X, W1), b1))\n",
    "    hidden_layer2 = tf.nn.softmax(tf.add(tf.matmul(hidden_layer1, W2), b2))\n",
    "    output_layer = (tf.add(tf.matmul(hidden_layer2, W3), b3))\n",
    "    \n",
    "    return output_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's Variable and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# All the economics and the environmental variable and functions\n",
    "\n",
    "t=1\n",
    "psi=0.69\n",
    "delta=0.0\n",
    "\n",
    "beta=1/(1+0.15)\n",
    "\n",
    "def utility(C):\n",
    "    return ((C/L)**(1-1/psi)-1)*L/(1-1/psi)\n",
    "    \n",
    "\n",
    "l=np.float32(7403)\n",
    "\n",
    "a=np.float32(5.115)\n",
    "\n",
    "Ci=0.308\n",
    "      \n",
    "ab=0.55*1000*2.6/3.666*0.00013418\n",
    "    \n",
    "nE=2.6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mitigation(m,teta=2.6):\n",
    "  \n",
    "    return ab*((10**(-6)+m)**teta)\n",
    "    \n",
    "\n",
    "def damage(T,p2=0.00235):\n",
    "    return 1/(1+p2*(T**2))\n",
    "\n",
    "def emissions(Y,m):\n",
    "   \n",
    "    return Ci*Y*(1-m)+nE\n",
    "def carbon_mass(Ma,E,phi=0.9942):\n",
    "    return phi*Ma+E\n",
    "\n",
    "\n",
    "def carbon_mass(Ma,E,phi=0.9942):\n",
    "    \n",
    "    return phi*Ma+E\n",
    "\n",
    "def temperature(T,Ma,mu0=-2.8672,mu1= 0.8954,mu2=0.4622):\n",
    "    \n",
    "\n",
    "    \n",
    "    return mu0+mu1*T+mu2*tf.math.log(Ma)\n",
    "\n",
    "\n",
    "def firm_net(K,m,T,A,L,alpha=0.3):\n",
    "    \n",
    "    env_factor=(1-mitigation(m,ab))*damage(T)\n",
    "    \n",
    "    r*A * K**(alpha-1)*(L/1000)**(1-alpha)\n",
    "\n",
    "    Y = env_factor*A * K**alpha*(L/1000)**(1-alpha)\n",
    "    \n",
    "    return  Y,r\n",
    "\n",
    "\n",
    "\n",
    "def firm(K,A,L,alpha=np.float32(0.3),delta=np.float32(0.)):\n",
    "    \n",
    "   \n",
    "    r= A * K**(alpha-1)*(L/1000)**(1-alpha)+(1-delta)\n",
    "\n",
    "    Y = A * K**alpha*(L/1000)**(1-alpha)\n",
    "    \n",
    "    return  Y,r\n",
    " \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tomorrow's State Giving the Current One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Today's  state: \n",
    "\n",
    "K = X[:,0] # Capital\n",
    "Ta = X[:,1]  # Temperature\n",
    "Ma = X[:,2]  # Carbon Mass\n",
    "r= X[:,3] # Cost of Capital\n",
    "Yg= X[:,4] # Gross Output\n",
    "L= X[:,5] # Labor Supply\n",
    "A= X[:,6] # Productivity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K = tf.expand_dims(K,axis=1)\n",
    "Ta =tf.expand_dims(Ta,axis=1) \n",
    " \n",
    "Ma = tf.expand_dims(Ma,axis=1)  \n",
    "r= tf.expand_dims(r,axis=1) \n",
    "Yg=tf.expand_dims(Yg,axis=1)\n",
    "A=tf.expand_dims(A,axis=1)\n",
    "L=tf.expand_dims(L,axis=1)\n",
    "\n",
    "#set capital greater than 100 to avoid a capital going to 0\n",
    "K_modif=tf.maximum(K,(100.+eps)*tf.ones_like(K))\n",
    "K_modif=tf.cast(K_modif,tf.float32)\n",
    "I=nn_predict(X)[:,0]\n",
    "I=tf.expand_dims(I,axis=1)\n",
    "I_modif=tf.maximum(I,eps*tf.ones_like(I))\n",
    "m=nn_predict(X)[:,1]\n",
    "m=tf.expand_dims(m,axis=1)\n",
    "V=nn_predict(X)[:,2]\n",
    "V=tf.expand_dims(V,axis=1)\n",
    "m_modif=tf.clip_by_value(m,eps,tf.constant(np.float32(1)))\n",
    "\n",
    "E=emissions(Yg,m_modif)\n",
    "\n",
    "Y_net,r_net_prime=firm_net(K_modif,m_modif,Ta,A,L)\n",
    "\n",
    "\n",
    "c=Y_net-I_modif\n",
    "\n",
    "c_modif=tf.maximum(c,tf.ones_like(c)*eps)\n",
    "c_modif=tf.cast(c_modif,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K_prime=I+(1.-delta)*K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ma_prime=carbon_mass(Ma,E)\n",
    "Ma_prime_modif=tf.maximum(Ma_prime,eps*tf.ones_like(Ma_prime))\n",
    "A_prime=a*tf.ones_like(A)\n",
    "L_prime=l*tf.ones_like(L)\n",
    "#Same for K_prime\n",
    "K_prime_modif=tf.maximum(K_prime,(100.+eps)*tf.ones_like(K_prime))\n",
    "Ta_prime=temperature(Ta,Ma_prime_modif)\n",
    "Ta_prime_modif=tf.maximum(Ta_prime,eps*tf.ones_like(Ta_prime))\n",
    "Yg_prime,r_prime=firm(K_prime_modif,A_prime,L_prime)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tomorrow's state\n",
    "X_prime=tf.concat([K_prime_modif,Ta_prime_modif,Ma_prime_modif,r_prime,Yg_prime,L_prime,A_prime],axis=1)\n",
    "\n",
    "out=nn_predict(X_prime)\n",
    "I_prime=out[:,0]\n",
    "m_prime=out[:,1]\n",
    "V_prime=out[:,2]\n",
    "V_prime=tf.expand_dims(V_prime,axis=1)\n",
    "m_prime=tf.expand_dims(m_prime,axis=1)\n",
    "m_prime_modif=tf.cast(tf.clip_by_value(m_prime,eps,np.float32(1.)),tf.float32)\n",
    "I_prime=tf.expand_dims(I_prime,axis=1)\n",
    "I_prime_modif=tf.maximum(I_prime,eps*tf.ones_like(I_prime))\n",
    "\n",
    "\n",
    "Y_net_prime,r_net_prime=firm_net(K_prime_modif,m_prime_modif,Ta_prime,A_prime,L_prime)\n",
    "\n",
    "\n",
    "c_prime=Y_net_prime-I_prime_modif\n",
    "c_prime_modif=tf.maximum(c_prime,tf.ones_like(c_prime)*eps)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Costs Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transitions to the next periods states. In this setting, there is a 25% chance\n",
    "# of ending up in any of the 4 states in Z. This has been hardcoded and need to be changed\n",
    "# to accomodate a different transition matrix.\n",
    "beta=1/(1+0.015)\n",
    "\n",
    "\n",
    "def marginal_utility(C,L,psi=0.69):\n",
    "    \n",
    "    return (C/L)**(-1/psi)\n",
    "\n",
    "def utility(C,L,psi=0.69):\n",
    "   \n",
    "    return ((C/L)**(1-1/psi)-1)/((1-1/psi))*L\n",
    "\n",
    "\n",
    "#Euler Equation\n",
    "\n",
    "opt_euler=(beta*marginal_utility(c_modif,L)*r_net_prime/marginal_utility(c_prime_modif,L) -1)\n",
    "\n",
    "#punish for negative capital\n",
    "opt_punish_capital =tf.maximum(100.-K_prime, tf.zeros_like(K_prime))\n",
    "\n",
    "opt_punish_capital = tf.cast(opt_punish_capital,tf.float32)\n",
    "\n",
    "#punish for negative consumption\n",
    "opt_punish_conso =  tf.maximum(- c, tf.zeros_like(c))\n",
    "opt_punish_conso_prime = tf.maximum(-c_prime, tf.zeros_like(c_prime))\n",
    "\n",
    "#punish for negative investment\n",
    "opt_punish_I =  tf.maximum(- I, tf.zeros_like(I))\n",
    "opt_punish_I_prime = tf.maximum(-I_prime, tf.zeros_like(I_prime))\n",
    "  \n",
    "#punish for decreasing capital \n",
    "\n",
    "opt_punish_incr =  tf.maximum(K_modif-K_prime_modif, tf.zeros_like(I))\n",
    "\n",
    "\n",
    "#punish if the mitigation is not below 0 and 1\n",
    "opt_punish_mu0 = tf.maximum(- m_prime, tf.zeros_like(m_prime))\n",
    "opt_punish_mu0 = tf.cast(opt_punish_mu0,tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "opt_punish_mu1 =  tf.maximum( m_prime-1, tf.zeros_like(m_prime))\n",
    "opt_punish_mu1 = tf.cast(opt_punish_mu1,tf.float32)\n",
    "\n",
    "\n",
    "opt_punish_mu0_or =tf.maximum(- m, tf.zeros_like(m))\n",
    "opt_punish_mu0_or = tf.cast(opt_punish_mu0_or,tf.float32)\n",
    "\n",
    "    \n",
    "opt_punish_mu1_or =  tf.maximum( m-1, tf.zeros_like(m))\n",
    "opt_punish_mu1_or = tf.cast(opt_punish_mu1_or,tf.float32)\n",
    "\n",
    "#punish for negative temperature    \n",
    "opt_punish_T =  tf.maximum( -Ta_prime, tf.zeros_like(Ta_prime))\n",
    "\n",
    "#Bellman Equation for the optimal path    \n",
    "opt_bell=((utility(c_modif,L)+beta*V_prime)/V)-1\n",
    "\n",
    "\n",
    "# We combine them and use then MSE for optimizer\n",
    "combined_opt = [ opt_punish_incr,opt_bell,opt_punish_capital,\n",
    "                opt_punish_I,opt_punish_I_prime ,opt_punish_conso, \n",
    "                opt_punish_conso_prime,opt_punish_mu0,opt_punish_mu1,opt_punish_mu0_or,opt_punish_mu1_or\n",
    "                           ]\n",
    "               \n",
    "               \n",
    "opt_predict = tf.concat(combined_opt, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "opt_correct = tf.zeros_like(opt_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Training Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue for clipping gradient with some None value\n",
    "def ClipIfNotNone(grad):\n",
    "    if grad is None:\n",
    "        return grad\n",
    "    return tf.clip_by_value(grad, -1, 1)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "cost = tf.losses.mean_squared_error(opt_correct, opt_predict)\n",
    "\n",
    "# Adam optimizer with a expenotential decaying learning rate because it is more stable\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1.\n",
    "decayed_lr = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                        global_step, 10000,\n",
    "                                        0.2, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= decayed_lr)\n",
    "\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "#capped_gvs = [(ClipIfNotNone(grad), var) for grad, var in gvs]\n",
    "  \n",
    "\n",
    "# Define a training step\n",
    "train_step = optimizer.apply_gradients(gvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we simulate the future states given the neural network and the first state\n",
    "\n",
    "\n",
    "def simulate_episodes(sess, x_start, episode_length, print_flag=True):\n",
    "\n",
    "    time_start = datetime.now()\n",
    "    if print_flag:\n",
    "        print('Start simulating {} periods.'.format(episode_length))\n",
    "    dim_state = np.shape(x_start)[1]\n",
    "\n",
    "    X_episodes = np.zeros([episode_length, dim_state])\n",
    "    X_episodes[0, :] = x_start\n",
    "    X_old = x_start\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    for t in range(1,episode_length):\n",
    "        \n",
    "        X_new = sess.run(X_prime, feed_dict={X: X_old})\n",
    "        \n",
    "        \n",
    "        # Append it to the dataset\n",
    "        X_episodes[t, :] = X_new\n",
    "    \n",
    "        X_old = (X_new)\n",
    "       \n",
    "\n",
    "    time_end = datetime.now()\n",
    "    time_diff = time_end - time_start\n",
    "    if print_flag:\n",
    "        print('Finished simulation. Time for simulation: {}.'.format(time_diff))\n",
    "\n",
    "    return X_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if data_path:\n",
    "    X_data_train = np.load(data_path)\n",
    "    print('Loaded initial data from ' + data_path)\n",
    "    start_episode = int(re.search('_(.*).npy', data_path).group(1))\n",
    "else:\n",
    "    # generate a random starting point with only positive values\n",
    "    X_data_train = np.random.uniform(1,2,(1,7))\n",
    " \n",
    "    X_data_train=np.float32(X_data_train)\n",
    "    \n",
    "  \n",
    "    start_episode = 0\n",
    "    \n",
    "if sess_path is not None:\n",
    "    saver.restore(sess, sess_path)\n",
    "   \n",
    "\n",
    "    train_seed = 0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "            \n",
    "for episode in range(start_episode, 5000):\n",
    "    \n",
    "    \n",
    "\n",
    "    X_episodes= simulate_episodes(sess, np.float32(X_data_train), 1000, print_flag=(episode==0))\n",
    "\n",
    "    X_data_train=X_episodes[-1:,]\n",
    "    \n",
    "    for epoch in range(epochs_per_episode):\n",
    "\n",
    "        train_seed += 1\n",
    "        minibatch_cost = 0\n",
    "\n",
    "       \n",
    "        minibatches = random_mini_batches(X_episodes, 30, train_seed)\n",
    "\n",
    "        for minibatch_X in minibatches:\n",
    "         \n",
    "            minibatch_cost += sess.run(cost, feed_dict={X: minibatch_X}) / num_minibatches\n",
    "           \n",
    "              \n",
    "      \n",
    "\n",
    "        for minibatch_X in minibatches:\n",
    "           \n",
    "            sess.run(train_step, feed_dict={X: minibatch_X})\n",
    "            cost_store.append(minibatch_cost)\n",
    "\n",
    "\n",
    "  \n",
    "   \n",
    "    print('Episode {}: Cost: {:.4f}'.format(episode,minibatch_cost ))\n",
    "                                                                                   \n",
    "                                                                                  \n",
    "\n",
    "    if episode%100==0:    \n",
    "\n",
    "        # Save the tensorflow session\n",
    "        saver.save(sess, './output/sess_{}.ckpt'.format(episode))\n",
    "        # Save the starting point\n",
    "        np.save ('./output/data_{}.npy'.format(episode), X_data_train)                                                                          \n",
    "                                                                                 "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc7a750a289689c1c8e28bd423c98e54548914f4d5d2e8756c150070c61d8b62"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
